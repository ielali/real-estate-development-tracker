# Story 6.2: User-Initiated Project Backup & Export

## Status

Approved

## Story

**As a** project owner,
**I want** to download a comprehensive backup of my project data,
**so that** I have a local copy of all project information for record-keeping and data portability.

## Acceptance Criteria

1. "Download Project Backup" button in project settings
2. Backup generates complete JSON with all project data
3. JSON is well-formatted and human-readable
4. Document metadata includes Netlify Blob URLs
5. Rate limiting enforced (5 backups/hour/user for JSON, 2/hour for ZIP)
6. Error message shown if rate limit exceeded
7. Backup history displayed (date, file size, backup type)
8. Only project owners can download backups (RBAC enforced)
9. Large projects (1000+ costs) export successfully (<10s)
10. Backup includes schema version for future compatibility
11. Two download options: "JSON only" and "Full archive with documents"
12. ZIP archive includes JSON file + all document files in organized folder structure
13. ZIP download shows file size estimate before initiating download
14. Clear UI indication of backup scope (metadata vs full archive)
15. Progress indicator for ZIP generation (async process)

## Tasks / Subtasks

- [ ] Create database schema for backup tracking (AC: 5, 7, 11)
  - [ ] Create `project_backups` table:
    - [ ] `id` (UUID primary key)
    - [ ] `projectId` (text, FK to projects.id with CASCADE)
    - [ ] `userId` (text, FK to users.id - who requested the backup)
    - [ ] `backupType` (text, enum: "json" | "zip" - type of backup)
    - [ ] `fileSize` (bigint, size in bytes)
    - [ ] `documentCount` (integer, number of documents included, 0 for JSON-only)
    - [ ] `schemaVersion` (text, e.g., "1.0.0")
    - [ ] `createdAt` (timestamp)
  - [ ] Run database migration:
    - [ ] Generate migration: `bunx drizzle-kit generate`
    - [ ] Review migration file in `drizzle/migrations/`
    - [ ] Apply migration: `bunx drizzle-kit push`
    - [ ] Verify schema in database

- [ ] Implement backup data aggregation service (AC: 2, 4, 9, 10)
  - [ ] Create `BackupService` class in `apps/web/src/server/services/backup.service.ts`
  - [ ] Implement `generateProjectBackup(projectId)` method
  - [ ] Aggregate data from all related tables:
    - [ ] Project details (name, description, dates, budget, status)
    - [ ] Address information (if project has address)
    - [ ] All costs with category information
    - [ ] All contacts with category information
    - [ ] All timeline events with category information
    - [ ] All documents with metadata (fileName, fileSize, mimeType, blobUrl, thumbnailUrl, category)
    - [ ] Project access/sharing settings (if applicable)
  - [ ] Use Drizzle ORM relations for efficient querying
  - [ ] Include schema version: "1.0.0" in backup JSON
  - [ ] Format JSON with `JSON.stringify(data, null, 2)` for readability
  - [ ] Optimize for large projects:
    - [ ] Use streaming for 1000+ records
    - [ ] Batch queries where appropriate
    - [ ] Add performance logging

- [ ] Implement ZIP archive generation service (AC: 11, 12, 13, 15)
  - [ ] Install JSZip library: `npm install jszip @types/jszip`
  - [ ] Extend `BackupService` with `generateZipArchive(projectId)` method
  - [ ] ZIP structure:
    - [ ] Root: `{project-name}-backup-{timestamp}.zip`
    - [ ] `/project-data.json` - Complete project metadata
    - [ ] `/documents/` folder with all document files
    - [ ] `/documents/{documentId}-{fileName}` - Original filename preserved
  - [ ] Fetch document blobs from Netlify Blobs:
    - [ ] Use `documentService.getDocumentBlob(documentId)` for each document
    - [ ] Handle missing blobs gracefully (skip and log warning)
    - [ ] Add documents to ZIP asynchronously
  - [ ] Calculate estimated ZIP size before generation:
    - [ ] Sum all document fileSizes from database
    - [ ] Add JSON size estimate (~50KB base + data)
    - [ ] Return size estimate to UI
  - [ ] Progress tracking:
    - [ ] Track documents processed / total documents
    - [ ] Return progress percentage for UI updates
    - [ ] Use streaming for large archives (>100MB)
  - [ ] Error handling:
    - [ ] Timeout after 60 seconds for very large projects
    - [ ] Graceful failure if blob retrieval fails
    - [ ] Clear error messages for UI

- [ ] Implement rate limiting mechanism (AC: 5, 6)
  - [ ] Create rate limiter utility in `apps/web/src/lib/rate-limiter.ts`
  - [ ] Track backup requests per user per hour by backup type
  - [ ] Store rate limit state in memory (simple Map) with per-type tracking
  - [ ] Configuration:
    - [ ] JSON backups: 5 per hour per user
    - [ ] ZIP backups: 2 per hour per user (more resource-intensive)
  - [ ] Return clear error when limit exceeded:
    - [ ] Error message: "Backup limit exceeded. You can download {limit} {type} backups per hour. Try again in X minutes."
    - [ ] Include remaining time in error response
    - [ ] Indicate which backup type limit was exceeded
  - [ ] Reset counter after 1 hour window

- [ ] Create tRPC backup endpoints (AC: 1, 2, 5, 8, 11, 12, 13, 15)
  - [ ] `projects.generateBackup` - Protected procedure (requires project ownership)
    - [ ] Input: `backupType: "json" | "zip"` (default: "json")
    - [ ] Verify user owns project (RBAC check)
    - [ ] Check rate limit based on backup type (throw error if exceeded)
    - [ ] Generate backup JSON using BackupService
    - [ ] Calculate file size
    - [ ] Record backup in `project_backups` table with backupType and documentCount
    - [ ] Return backup JSON + filename
  - [ ] `projects.generateZipBackup` - Protected procedure (requires project ownership)
    - [ ] Verify user owns project (RBAC check)
    - [ ] Check ZIP rate limit (2 per hour)
    - [ ] Generate ZIP archive using BackupService
    - [ ] Calculate file size
    - [ ] Record backup in `project_backups` table (type: "zip")
    - [ ] Return ZIP blob as base64 or stream + filename
    - [ ] Support progress callback for UI updates
  - [ ] `projects.estimateZipSize` - Protected procedure
    - [ ] Verify user owns project
    - [ ] Calculate estimated ZIP size (sum of document fileSizes + JSON)
    - [ ] Return: estimatedSize (bytes), documentCount, warningMessage (if >100MB)
  - [ ] `projects.getBackupHistory` - Protected procedure
    - [ ] Verify user owns project
    - [ ] Fetch last 10 backups from `project_backups` table
    - [ ] Return: createdAt, fileSize, schemaVersion, backupType, documentCount
  - [ ] Use `TRPCError` with appropriate codes:
    - [ ] `FORBIDDEN` - User doesn't own project
    - [ ] `TOO_MANY_REQUESTS` - Rate limit exceeded (with backup type specified)

- [ ] Create backup UI components (AC: 1, 6, 7, 11, 13, 14, 15)
  - [ ] `ProjectBackupSection` component for project settings
    - [ ] Two download options with clear labeling:
      - [ ] "JSON Only" button - "Metadata only (fastest, ~50 KB)"
      - [ ] "Full Archive" button - "With documents (includes all files, ~{size estimate})"
    - [ ] Size estimation display for ZIP option:
      - [ ] Show estimated size before download
      - [ ] Warning badge if size >100MB ("Large download")
      - [ ] Document count display (e.g., "Includes 45 documents")
    - [ ] Loading states during backup generation:
      - [ ] JSON: Simple spinner with "Generating backup..."
      - [ ] ZIP: Progress bar with "Packaging documents... {X}/{Y}"
    - [ ] Error state display (rate limit, network errors)
    - [ ] Success toast notification on backup download
    - [ ] Information alert explaining backup scope differences
  - [ ] `BackupHistoryList` component
    - [ ] Display last 10 backups in table/list
    - [ ] Show: date, file size (formatted: KB/MB), schema version, backup type badge, document count
    - [ ] Badge for backup type: "JSON" (blue) vs "ZIP" (green)
    - [ ] Responsive design (stack on mobile)
  - [ ] Follow Shadcn/ui patterns
    - [ ] Use Button component with loading spinner
    - [ ] Use Alert component for info/errors
    - [ ] Use Table component for backup history
    - [ ] Use Toast for success notifications (Sonner library)
    - [ ] Use Progress component for ZIP generation
    - [ ] Use Badge component for backup type indicators

- [ ] Implement backup download functionality (AC: 3, 4, 9, 10, 11, 12, 15)
  - [ ] Add project settings page route if it doesn't exist
  - [ ] Create `/projects/[id]/settings` page component
  - [ ] Integrate `ProjectBackupSection` and `BackupHistoryList`
  - [ ] On JSON backup generation success:
    - [ ] Trigger browser download with descriptive filename
    - [ ] Filename format: `{project-name}-backup-{YYYY-MM-DD-HHmmss}.json`
    - [ ] Sanitize project name for filename (remove special chars, spaces)
    - [ ] Use `application/json` MIME type
  - [ ] On ZIP backup generation success:
    - [ ] Trigger browser download with descriptive filename
    - [ ] Filename format: `{project-name}-archive-{YYYY-MM-DD-HHmmss}.zip`
    - [ ] Use `application/zip` MIME type
    - [ ] Handle large file downloads (>100MB)
  - [ ] Handle download trigger:
    - [ ] Create blob from JSON string or ZIP buffer
    - [ ] Use `URL.createObjectURL` for download link
    - [ ] Programmatically click link to trigger download
    - [ ] Clean up blob URL after download
  - [ ] Progress tracking for ZIP downloads:
    - [ ] Poll tRPC endpoint for progress updates
    - [ ] Update progress bar in real-time
    - [ ] Allow cancellation (optional enhancement)

- [ ] Ensure RBAC enforcement (AC: 8)
  - [ ] Verify ownership check in tRPC procedures
  - [ ] Test with non-owner user (should return FORBIDDEN error)
  - [ ] Test with project owner (should succeed)
  - [ ] UI should only show backup section to project owners
  - [ ] Hide backup functionality from partners/viewers

- [ ] Write tests for backup functionality
  - [ ] Unit test: BackupService data aggregation (JSON)
  - [ ] Unit test: BackupService ZIP archive generation
  - [ ] Unit test: Rate limiter logic (within limit, exceeded limit, reset after hour, per-type tracking)
  - [ ] Unit test: ZIP size estimation accuracy
  - [ ] Integration test: Generate JSON backup for small project
  - [ ] Integration test: Generate JSON backup for large project (1000+ costs)
  - [ ] Integration test: Generate ZIP backup with documents
  - [ ] Integration test: ZIP backup handles missing blobs gracefully
  - [ ] Integration test: Rate limiting enforcement (JSON: 5/hr, ZIP: 2/hr)
  - [ ] Integration test: RBAC enforcement (owner vs non-owner)
  - [ ] Integration test: Backup history retrieval shows both types
  - [ ] Integration test: Size estimation endpoint accuracy
  - [ ] Component test: ProjectBackupSection dual options UI
  - [ ] Component test: ProjectBackupSection progress tracking (ZIP)
  - [ ] Component test: BackupHistoryList displays type badges
  - [ ] E2E test: Full ZIP download workflow with progress

## Dev Notes

### Existing System Context

**Database Structure:**
[Source: Database schema files in [apps/web/src/server/db/schema/](apps/web/src/server/db/schema/)]

- **Projects:** `id`, `name`, `description`, `addressId`, `projectType`, `status`, `startDate`, `endDate`, `ownerId`, `totalBudget`, `createdAt`, `updatedAt`, `deletedAt`
- **Costs:** `id`, `projectId`, `amount`, `description`, `categoryId`, `date`, `contactId`, `documentIds`, `createdById`, `createdAt`, `updatedAt`, `deletedAt`
- **Contacts:** `id`, `firstName`, `lastName`, `company`, `email`, `phone`, `mobile`, `website`, `addressId`, `categoryId`, `notes`, `createdAt`, `updatedAt`, `deletedAt`
- **Events (Timeline):** `id`, `projectId`, `title`, `description`, `date`, `categoryId`, `createdById`, `createdAt`, `updatedAt`, `deletedAt`
- **Documents:** `id`, `projectId`, `fileName`, `fileSize`, `mimeType`, `blobUrl`, `thumbnailUrl`, `categoryId`, `uploadedById`, `createdAt`, `updatedAt`, `deletedAt`
- **Addresses:** Referenced by projects and contacts
- **Categories:** Referenced by costs, contacts, events, documents

**Document Storage:**
[Source: [apps/web/src/server/services/document.service.ts](apps/web/src/server/services/document.service.ts)]

- Netlify Blobs storage for document files
- `blobUrl` field contains Netlify Blob URL for file access
- Documents table stores metadata only (not file contents)
- Backup should include blobUrl for document access, not file contents

**RBAC Implementation:**

- Projects have `ownerId` field (FK to users.id)
- Project ownership check: `project.ownerId === ctx.user.id`
- Project access also managed via `projectAccess` table (for partners/viewers)

**Tech Stack:**
[Source: [docs/architecture/tech-stack.md](docs/architecture/tech-stack.md)]

- Next.js 14 with App Router
- TypeScript 5.3
- tRPC ^10.45.0 for API layer
- Drizzle ORM ^0.44.6 for database
- Neon PostgreSQL (serverless)
- Shadcn/ui + Tailwind CSS for UI
- Sonner for toast notifications

**New Dependencies Required:**

```bash
npm install jszip @types/jszip
```

- **JSZip** (^3.10.1): Client and server-side ZIP archive generation
  - Lightweight, pure JavaScript implementation
  - Supports compression, streaming, and progress tracking
  - Browser and Node.js compatible
  - License: MIT or GPLv3

### File Locations

**Database Schema:**

- New table: `apps/web/src/server/db/schema/project-backups.ts`
- Export from: `apps/web/src/server/db/schema/index.ts`

**Services:**

- BackupService: `apps/web/src/server/services/backup.service.ts`

**Utilities:**

- Rate limiter: `apps/web/src/lib/rate-limiter.ts`

**tRPC Router:**

- Extend projects router: `apps/web/src/server/api/routers/projects.ts`
- Add endpoints: `generateBackup`, `getBackupHistory`

**Components:** (create in `apps/web/src/components/projects/settings/`)

- `ProjectBackupSection.tsx` - Main backup download section
- `BackupHistoryList.tsx` - Table of recent backups

**Pages:**

- Project settings: `apps/web/src/app/projects/[id]/settings/page.tsx`

**Tests:**

- BackupService tests: `apps/web/src/server/services/__tests__/backup.service.test.ts`
- Rate limiter tests: `apps/web/src/lib/__tests__/rate-limiter.test.ts`
- tRPC endpoint tests: `apps/web/src/server/api/routers/__tests__/projects-backup.test.ts`
- Component tests: Co-located in `apps/web/src/components/projects/settings/__tests__/`

### Technical Implementation Notes

**Backup JSON Structure:**

```typescript
interface ProjectBackup {
  version: string // Schema version "1.0.0"
  exportedAt: string // ISO timestamp
  exportedBy: {
    id: string
    email: string
    name: string
  }
  project: {
    id: string
    name: string
    description: string | null
    projectType: string
    status: string
    startDate: string | null
    endDate: string | null
    totalBudget: number | null
    address: Address | null
    createdAt: string
    updatedAt: string
  }
  costs: Array<{
    id: string
    amount: number
    description: string
    category: { id: string; name: string }
    date: string
    contact: Contact | null
    documentIds: string[] | null
    createdBy: { id: string; email: string; name: string }
    createdAt: string
  }>
  contacts: Contact[]
  events: Event[]
  documents: Array<{
    id: string
    fileName: string
    fileSize: number
    mimeType: string
    blobUrl: string // Netlify Blob URL for file access
    thumbnailUrl: string | null
    category: { id: string; name: string }
    uploadedBy: { id: string; email: string; name: string }
    createdAt: string
  }>
}
```

**Rate Limiter Implementation:**

```typescript
// Simple in-memory rate limiter
interface RateLimitEntry {
  count: number
  resetAt: number // Unix timestamp
}

class RateLimiter {
  private limits = new Map<string, RateLimitEntry>()

  check(userId: string, maxRequests: number, windowMs: number): boolean {
    const now = Date.now()
    const entry = this.limits.get(userId)

    // No entry or expired window
    if (!entry || now > entry.resetAt) {
      this.limits.set(userId, {
        count: 1,
        resetAt: now + windowMs,
      })
      return true
    }

    // Within window, check count
    if (entry.count < maxRequests) {
      entry.count++
      return true
    }

    return false // Limit exceeded
  }

  getTimeUntilReset(userId: string): number {
    const entry = this.limits.get(userId)
    if (!entry) return 0
    return Math.max(0, entry.resetAt - Date.now())
  }
}

export const backupRateLimiter = new RateLimiter()

// Usage in tRPC procedure
const canProceed = backupRateLimiter.check(
  ctx.user.id,
  5, // max 5 requests
  60 * 60 * 1000 // per hour
)

if (!canProceed) {
  const resetIn = backupRateLimiter.getTimeUntilReset(ctx.user.id)
  const minutesRemaining = Math.ceil(resetIn / 1000 / 60)
  throw new TRPCError({
    code: "TOO_MANY_REQUESTS",
    message: `Backup limit exceeded. You can download 5 backups per hour. Try again in ${minutesRemaining} minutes.`,
  })
}
```

**Backup Generation with Drizzle Relations:**

```typescript
import { db } from "../db"
import { projects, costs, contacts, events, documents } from "../db/schema"

export class BackupService {
  async generateProjectBackup(projectId: string, userId: string) {
    // Use Drizzle relational queries for efficiency
    const projectData = await db.query.projects.findFirst({
      where: (projects, { eq, and, isNull }) =>
        and(eq(projects.id, projectId), isNull(projects.deletedAt)),
      with: {
        address: true,
        costs: {
          where: (costs, { isNull }) => isNull(costs.deletedAt),
          with: {
            category: true,
            contact: {
              with: { category: true, address: true },
            },
            createdBy: true,
          },
        },
        events: {
          where: (events, { isNull }) => isNull(events.deletedAt),
          with: {
            category: true,
            createdBy: true,
          },
        },
        documents: {
          where: (documents, { isNull }) => isNull(documents.deletedAt),
          with: {
            category: true,
            uploadedBy: true,
          },
        },
      },
    })

    if (!projectData) {
      throw new Error("Project not found")
    }

    // Get all contacts related to this project
    // Aggregate unique contact IDs from costs
    const contactIdsInCosts = projectData.costs
      .filter((cost) => cost.contactId)
      .map((cost) => cost.contactId!)
      .filter((id, index, self) => self.indexOf(id) === index) // Unique IDs

    // Fetch contact details if any contacts are referenced
    const projectContacts =
      contactIdsInCosts.length > 0
        ? await db.query.contacts.findMany({
            where: (contacts, { and, isNull, inArray }) =>
              and(isNull(contacts.deletedAt), inArray(contacts.id, contactIdsInCosts)),
            with: {
              category: true,
              address: true,
            },
          })
        : [] // No contacts if none referenced in costs

    const backup: ProjectBackup = {
      version: "1.0.0",
      exportedAt: new Date().toISOString(),
      exportedBy: {
        id: userId,
        // Fetch user details...
      },
      project: {
        // Map projectData...
      },
      costs: projectData.costs.map((cost) => ({
        // Map cost data...
      })),
      contacts: projectContacts,
      events: projectData.events,
      documents: projectData.documents.map((doc) => ({
        id: doc.id,
        fileName: doc.fileName,
        fileSize: doc.fileSize,
        mimeType: doc.mimeType,
        blobUrl: doc.blobUrl, // Include Netlify Blob URL
        thumbnailUrl: doc.thumbnailUrl,
        category: doc.category,
        uploadedBy: doc.uploadedBy,
        createdAt: doc.createdAt.toISOString(),
      })),
    }

    return backup
  }
}
```

**File Download Trigger (Client-side):**

```typescript
function downloadBackup(backupData: ProjectBackup, projectName: string) {
  // Format timestamp
  const now = new Date()
  const timestamp = now.toISOString().replace(/:/g, "").replace(/\..+/, "").replace("T", "-")

  // Sanitize project name for filename
  const sanitizedName = projectName
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, "-")
    .replace(/^-+|-+$/g, "")

  const filename = `${sanitizedName}-backup-${timestamp}.json`

  // Create JSON string with formatting
  const jsonString = JSON.stringify(backupData, null, 2)

  // Create blob and trigger download
  const blob = new Blob([jsonString], { type: "application/json" })
  const url = URL.createObjectURL(blob)

  const link = document.createElement("a")
  link.href = url
  link.download = filename
  document.body.appendChild(link)
  link.click()

  // Cleanup
  document.body.removeChild(link)
  URL.revokeObjectURL(url)
}
```

**ZIP Archive Generation with JSZip:**

```typescript
import JSZip from "jszip"
import { documentService } from "./document.service"

export class BackupService {
  // ... existing generateProjectBackup method ...

  /**
   * Generate ZIP archive with project data and all document files
   * @param projectId - Project UUID
   * @param userId - User requesting the backup
   * @param onProgress - Optional callback for progress tracking
   * @returns ZIP blob ready for download
   */
  async generateZipArchive(
    projectId: string,
    userId: string,
    onProgress?: (current: number, total: number) => void
  ): Promise<Blob> {
    // 1. Generate JSON backup data
    const backupData = await this.generateProjectBackup(projectId, userId)
    const jsonString = JSON.stringify(backupData, null, 2)

    // 2. Create new ZIP archive
    const zip = new JSZip()

    // 3. Add JSON file to root
    zip.file("project-data.json", jsonString)

    // 4. Create documents folder
    const docsFolder = zip.folder("documents")
    if (!docsFolder) {
      throw new Error("Failed to create documents folder in ZIP")
    }

    // 5. Fetch and add all document files
    const documents = backupData.documents
    const totalDocs = documents.length
    let processedDocs = 0

    for (const doc of documents) {
      try {
        // Fetch document blob from Netlify Blobs
        const blobData = await documentService.getDocumentBlob(doc.id)

        // Convert base64 string to binary
        const binaryData = Buffer.from(blobData, "base64")

        // Add to ZIP with original filename
        // Format: {documentId}-{fileName} to ensure uniqueness
        const zipFileName = `${doc.id}-${doc.fileName}`
        docsFolder.file(zipFileName, binaryData)

        // Update progress
        processedDocs++
        if (onProgress) {
          onProgress(processedDocs, totalDocs)
        }
      } catch (error) {
        // Log error but continue with other documents
        console.error(`Failed to fetch document ${doc.id}:`, error)
        // Optionally: Add error log file to ZIP
        docsFolder.file(
          `${doc.id}-MISSING.txt`,
          `Document ${doc.fileName} could not be retrieved. Error: ${error.message}`
        )

        processedDocs++
        if (onProgress) {
          onProgress(processedDocs, totalDocs)
        }
      }
    }

    // 6. Generate ZIP blob
    const zipBlob = await zip.generateAsync({
      type: "blob",
      compression: "DEFLATE",
      compressionOptions: { level: 6 }, // Balance between size and speed
    })

    return zipBlob
  }

  /**
   * Estimate ZIP archive size before generation
   * @param projectId - Project UUID
   * @returns Estimated size in bytes and document count
   */
  async estimateZipSize(projectId: string): Promise<{
    estimatedSize: number
    documentCount: number
    warningMessage?: string
  }> {
    // Fetch project with documents
    const projectData = await db.query.projects.findFirst({
      where: (projects, { eq, and, isNull }) =>
        and(eq(projects.id, projectId), isNull(projects.deletedAt)),
      with: {
        documents: {
          where: (documents, { isNull }) => isNull(documents.deletedAt),
        },
      },
    })

    if (!projectData) {
      throw new Error("Project not found")
    }

    // Calculate total document size
    const totalDocSize = projectData.documents.reduce((sum, doc) => sum + (doc.fileSize || 0), 0)

    // Estimate JSON size (roughly 1KB per cost/contact/event + overhead)
    const estimatedJsonSize = 50 * 1024 // 50KB base estimate

    // Total estimate (ZIP compression typically achieves 20-40% reduction)
    // Using conservative 30% compression ratio
    const estimatedSize = Math.round((totalDocSize + estimatedJsonSize) * 0.7)

    const documentCount = projectData.documents.length

    // Generate warning for large archives
    let warningMessage: string | undefined
    if (estimatedSize > 100 * 1024 * 1024) {
      warningMessage = "This archive will be over 100MB. Download may take several minutes."
    } else if (documentCount > 100) {
      warningMessage = "This archive contains many documents. Generation may take 1-2 minutes."
    }

    return { estimatedSize, documentCount, warningMessage }
  }
}
```

**Updated Rate Limiter with Per-Type Tracking:**

```typescript
// Enhanced rate limiter with backup type support
interface RateLimitEntry {
  count: number
  resetAt: number
}

interface TypedRateLimitEntry {
  json: RateLimitEntry | null
  zip: RateLimitEntry | null
}

class TypedRateLimiter {
  private limits = new Map<string, TypedRateLimitEntry>()

  check(
    userId: string,
    backupType: "json" | "zip",
    maxRequests: number,
    windowMs: number
  ): boolean {
    const now = Date.now()
    let userEntry = this.limits.get(userId)

    // Initialize user entry if doesn't exist
    if (!userEntry) {
      userEntry = { json: null, zip: null }
      this.limits.set(userId, userEntry)
    }

    const entry = userEntry[backupType]

    // No entry or expired window
    if (!entry || now > entry.resetAt) {
      userEntry[backupType] = {
        count: 1,
        resetAt: now + windowMs,
      }
      return true
    }

    // Within window, check count
    if (entry.count < maxRequests) {
      entry.count++
      return true
    }

    return false // Limit exceeded
  }

  getTimeUntilReset(userId: string, backupType: "json" | "zip"): number {
    const userEntry = this.limits.get(userId)
    if (!userEntry) return 0

    const entry = userEntry[backupType]
    if (!entry) return 0

    return Math.max(0, entry.resetAt - Date.now())
  }
}

export const backupRateLimiter = new TypedRateLimiter()

// Usage in tRPC procedure
const backupType = input.backupType // 'json' | 'zip'
const maxRequests = backupType === "json" ? 5 : 2

const canProceed = backupRateLimiter.check(
  ctx.user.id,
  backupType,
  maxRequests,
  60 * 60 * 1000 // per hour
)

if (!canProceed) {
  const resetIn = backupRateLimiter.getTimeUntilReset(ctx.user.id, backupType)
  const minutesRemaining = Math.ceil(resetIn / 1000 / 60)
  throw new TRPCError({
    code: "TOO_MANY_REQUESTS",
    message: `${backupType.toUpperCase()} backup limit exceeded. You can download ${maxRequests} ${backupType} backups per hour. Try again in ${minutesRemaining} minutes.`,
  })
}
```

**Client-Side ZIP Download with Progress:**

```typescript
'use client';

import { useState } from 'react';
import { api } from '~/trpc/react';
import { Progress } from '@/components/ui/progress';
import { Button } from '@/components/ui/button';

export function ProjectBackupSection({ projectId }: { projectId: string }) {
  const [zipProgress, setZipProgress] = useState(0);
  const [isGenerating, setIsGenerating] = useState(false);

  // Estimate ZIP size
  const { data: sizeEstimate } = api.projects.estimateZipSize.useQuery(
    { projectId },
    { enabled: true }
  );

  // Generate ZIP backup mutation
  const generateZip = api.projects.generateZipBackup.useMutation({
    onSuccess: (data) => {
      // Trigger download
      const blob = new Blob([Buffer.from(data.zipData, 'base64')], {
        type: 'application/zip',
      });
      const url = URL.createObjectURL(blob);

      const link = document.createElement('a');
      link.href = url;
      link.download = data.filename;
      document.body.appendChild(link);
      link.click();

      document.body.removeChild(link);
      URL.revokeObjectURL(url);

      setIsGenerating(false);
      setZipProgress(0);
    },
    onError: (error) => {
      console.error('ZIP generation failed:', error);
      setIsGenerating(false);
      setZipProgress(0);
    },
  });

  const handleZipDownload = async () => {
    setIsGenerating(true);
    setZipProgress(0);

    // Simulate progress tracking (in production, use tRPC subscriptions or polling)
    const progressInterval = setInterval(() => {
      setZipProgress((prev) => {
        if (prev >= 90) return prev; // Cap at 90% until complete
        return prev + 10;
      });
    }, 500);

    try {
      await generateZip.mutateAsync({ projectId });
      setZipProgress(100);
    } finally {
      clearInterval(progressInterval);
    }
  };

  return (
    <div className="space-y-4">
      <h3>Download Project Backup</h3>

      {/* Size estimate display */}
      {sizeEstimate && (
        <div className="text-sm text-muted-foreground">
          Includes {sizeEstimate.documentCount} documents (
          ~{Math.round(sizeEstimate.estimatedSize / 1024 / 1024)}MB)
          {sizeEstimate.warningMessage && (
            <p className="text-amber-600 mt-1">{sizeEstimate.warningMessage}</p>
          )}
        </div>
      )}

      {/* Download buttons */}
      <div className="flex gap-3">
        <Button variant="outline">
          JSON Only
          <span className="text-xs text-muted-foreground ml-2">~50KB</span>
        </Button>

        <Button
          onClick={handleZipDownload}
          disabled={isGenerating}
        >
          {isGenerating ? 'Generating...' : 'Full Archive'}
          {sizeEstimate && !isGenerating && (
            <span className="text-xs ml-2">
              ~{Math.round(sizeEstimate.estimatedSize / 1024 / 1024)}MB
            </span>
          )}
        </Button>
      </div>

      {/* Progress bar for ZIP generation */}
      {isGenerating && (
        <div className="space-y-2">
          <Progress value={zipProgress} className="w-full" />
          <p className="text-sm text-muted-foreground">
            Packaging documents... {zipProgress}%
          </p>
        </div>
      )}
    </div>
  );
}
```

### Performance Considerations

**Large Project Optimization (JSON Backups):**

For projects with 1000+ costs:

- Use Drizzle's efficient relational queries (single query with joins)
- Consider pagination if backup size exceeds memory limits
- Add performance logging to track generation time
- Target: <10 seconds for 1000+ cost entries
- Compress JSON if file size > 5MB (optional enhancement)

**ZIP Archive Optimization:**

For projects with many documents:

- Fetch blobs in parallel (limit concurrency to 5-10 to avoid overwhelming Netlify Blobs)
- Use streaming ZIP generation for archives >100MB (JSZip supports `generateAsync` with streaming)
- Set timeout of 60 seconds for very large projects (fail gracefully)
- Compression level 6 balances speed vs size (range: 1=fastest, 9=smallest)
- Memory considerations:
  - Each document loaded into memory during ZIP generation
  - For 100 documents Ã— 5MB avg = 500MB peak memory usage
  - Consider server-side generation with streaming for very large projects

**Rate Limiting Strategy:**

- In-memory Map sufficient for MVP (single server instance)
- Separate limits for JSON (5/hr) and ZIP (2/hr) prevents resource exhaustion
- ZIP backups are 10-50x more resource-intensive than JSON
- For production at scale, consider Redis for distributed rate limiting
- Cleanup stale entries periodically to prevent memory leak

**Estimated Generation Times:**

- JSON backup: <5 seconds (even for large projects)
- ZIP backup:
  - Small (10 docs, 20MB): ~5-10 seconds
  - Medium (50 docs, 100MB): ~20-30 seconds
  - Large (100+ docs, 200MB+): ~45-60 seconds
- Network transfer time additional (depends on user's bandwidth)

### Existing Patterns to Follow

**Form Components:**
[Source: [docs/architecture/coding-standards.md](docs/architecture/coding-standards.md)]

- Use React Hook Form + Zod validation
- Shadcn/ui components for consistency
- Server-side validation on all mutations
- Display inline errors with FormMessage

**Error Handling:**

- Use TRPCError with appropriate codes
- `FORBIDDEN` for ownership violations
- `TOO_MANY_REQUESTS` for rate limit exceeded
- `NOT_FOUND` for missing projects
- Clear, actionable error messages

**Database Patterns:**

- Use Drizzle ORM relational queries
- Exclude soft-deleted records (WHERE deletedAt IS NULL)
- Use transactions for multi-step operations
- Server-side Zod validation required

### Security Considerations

**Access Control:**

- RBAC enforcement: Only project owners can generate backups
- Verify ownership before generating backup
- Backup history shows only user's own backups
- Rate limiting prevents abuse

**Data Exposure:**

- **JSON backups:** Include all project metadata (costs, contacts, events, document metadata)
  - Document files NOT included (only metadata with blobUrl)
- **ZIP backups:** Include all project data PLUS actual document files
  - Full data portability but larger security concern
  - Contains financial data AND potentially sensitive documents (contracts, invoices, etc.)
- Sensitive information warning required in UI before download
- Users responsible for securing downloaded backup files (especially ZIP archives)
- Consider encryption for ZIP archives in future enhancement (password-protected ZIP)

**Rate Limiting:**

- **JSON backups:** 5 per hour per user
- **ZIP backups:** 2 per hour per user (more restrictive due to resource intensity)
- Prevents:
  - Server resource abuse (CPU, memory, bandwidth)
  - Excessive database and blob storage queries
  - Potential data scraping or bulk export
- Clear error messages guide users and indicate which limit was exceeded

### Risk Mitigation

**Primary Risks:**

1. **Database Performance:** Large backup generation could impact database performance
2. **Memory Exhaustion:** ZIP generation with many large documents could exhaust server memory
3. **Blob Storage Costs:** Frequent ZIP downloads increase Netlify Blobs read operations and bandwidth
4. **Data Security:** ZIP archives contain sensitive files that could be exposed if not secured

**Mitigation Strategies:**

1. **Database optimization:**
   - Use Drizzle relational queries (minimize N+1 queries)
   - Add timeout protection (10s for JSON, 60s for ZIP)
   - Performance monitoring and logging
2. **Memory management:**
   - Process documents sequentially for ZIP generation (not all at once)
   - Set hard limit on ZIP size (e.g., 500MB max)
   - Graceful failure if memory threshold exceeded
3. **Rate limiting:**
   - JSON: 5/hour prevents excessive database queries
   - ZIP: 2/hour prevents blob storage abuse and memory issues
   - Track both types separately
4. **Security:**
   - Clear UI warnings about data sensitivity
   - RBAC enforcement (owners only)
   - Optional: Add password protection to ZIP archives (future enhancement)
5. **Testing:**
   - Test with large datasets (1000+ costs, 100+ documents)
   - Load testing for concurrent ZIP generations
   - Memory profiling for large archives

**Rollback Plan:**

- Feature can be hidden via UI flag if performance issues arise
- Database table is additive only (no breaking changes)
- Can disable ZIP functionality independently from JSON backups
- Can adjust rate limits dynamically without code changes
- Can disable rate limiter if too restrictive

### Testing

**Test Environment:**

- Backend tests: `apps/web/src/server/services/__tests__/backup.service.test.ts`
- Integration tests: `apps/web/src/server/api/routers/__tests__/projects-backup.test.ts`
- Component tests: Co-located with components
- Use Vitest for all tests

**Test Coverage Required:**

**JSON Backups:**

1. BackupService generates complete, valid JSON
2. Backup includes all project data (costs, contacts, events, documents metadata)
3. Document blobUrl included in backup (not file contents)
4. Schema version "1.0.0" present in backup
5. Large project (1000+ costs) completes in <10s
6. Filename formatting is correct (sanitized, timestamped)

**ZIP Backups:** 7. BackupService generates valid ZIP archive 8. ZIP contains project-data.json file at root 9. ZIP contains documents/ folder with all document files 10. Document filenames in ZIP are correctly formatted ({id}-{fileName}) 11. Missing blobs handled gracefully (error file created in ZIP) 12. Size estimation is accurate (within 20% of actual size) 13. Progress tracking callback is invoked correctly 14. ZIP compression level 6 is applied 15. Large ZIP (100+ documents) generates successfully

**Rate Limiting:** 16. JSON rate limiter allows 5 requests within hour 17. JSON rate limiter blocks 6th request 18. ZIP rate limiter allows 2 requests within hour 19. ZIP rate limiter blocks 3rd request 20. Rate limiters track JSON and ZIP separately 21. Rate limiters reset after 1 hour

**RBAC:** 22. Project owner can generate JSON backup 23. Project owner can generate ZIP backup 24. Non-owner cannot generate backup (FORBIDDEN error) 25. Backup history shows only user's own backups

**UI & Integration:** 26. Backup history shows last 10 backups with type badges 27. Size estimation endpoint returns correct values 28. Progress bar updates during ZIP generation 29. Download triggers correctly for both JSON and ZIP 30. Error messages display correctly for rate limits

**Testing Frameworks:**
[Source: [docs/architecture/tech-stack.md](docs/architecture/tech-stack.md)]

- Vitest for backend unit/integration tests
- Testing Library for component tests
- Mock Drizzle queries for unit tests
- Use test database for integration tests

## Change Log

| Date       | Version | Description                                                                                                                                                                                                                                         | Author |
| ---------- | ------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------ |
| 2025-10-31 | 1.2     | Major enhancement: Added ZIP archive option with full document files. Updated AC (11-15), tasks, rate limiting (JSON: 5/hr, ZIP: 2/hr), JSZip dependency, comprehensive implementation examples, security considerations, and testing requirements. | Sarah  |
| 2025-10-31 | 1.1     | Enhancement: Completed contacts query example in BackupService code (lines 363-383)                                                                                                                                                                 | Sarah  |
| 2025-10-31 | 1.0     | Initial story creation from Epic 6 with proactive validation fixes applied                                                                                                                                                                          | Sarah  |

## Dev Agent Record

### Agent Model Used

(To be populated by dev agent)

### Debug Log References

(To be populated by dev agent)

### Completion Notes List

(To be populated by dev agent)

### File List

(To be populated by dev agent)

## QA Results

### Review Date: 2025-11-01

### Reviewed By: Quinn (Test Architect)

### âš ï¸ CRITICAL: Story Not Ready for Review

**Prerequisites Violated:**

- âŒ Story status is "Approved" (should be "Review")
- âŒ File List is empty in Dev Agent Record
- âŒ No tests written (0 test files found)
- âŒ Database migration not executed

**Implementation Status: ~40% Complete (Backend Only)**

This appears to be **WORK IN PROGRESS**, not a completed story ready for QA review. Review performed on partial implementation to provide early feedback.

### Code Quality Assessment

**Positive Aspects:**

- âœ… Excellent architectural decisions - proper use of Drizzle relational queries
- âœ… Strong TypeScript typing throughout
- âœ… Well-designed schema with proper versioning (1.0.0)
- âœ… Good separation of concerns (service layer, router layer)
- âœ… Rate limiting design is sound (separate limits for JSON/ZIP)
- âœ… RBAC enforcement correctly implemented in all endpoints
- âœ… Error handling structure is mostly comprehensive

**Critical Issues Found:**

- ðŸ› **BUG-001**: Critical bug in [backup.service.ts:591](apps/web/src/server/services/backup.service.ts#L591) - `getDocumentBlob()` called with `doc.id` instead of `doc.blobUrl`. This will cause ZIP downloads to fail.
- âŒ **TEST-001**: Zero test coverage (0/30 required test cases)
- âŒ **IMPL-001**: UI components completely missing (ProjectBackupSection, BackupHistoryList)
- âŒ **IMPL-002**: Database migration not run (schema created but not applied)
- âš ï¸ **PERF-001**: No timeout protection for large backup generation (AC 9 requires <10s)
- âš ï¸ **PERF-002**: No hard limit on ZIP archive size (memory exhaustion risk)
- âš ï¸ **ARCH-001**: In-memory rate limiter assumes single server (not distributed)

### Refactoring Performed

**None** - As QA agent, I performed analysis only. No code refactoring was done due to critical blocking issues requiring developer attention.

### Compliance Check

- **Coding Standards**: âš ï¸ Partially Compliant
  - âœ… TypeScript standards followed
  - âœ… Naming conventions correct
  - âš ï¸ Missing JSDoc comments for some functions
  - âœ… Good file organization

- **Project Structure**: âœ… Compliant
  - Files created in correct locations per story specification
  - Schema properly exported from index.ts
  - Relations properly defined

- **Testing Strategy**: âŒ Not Compliant
  - **0 tests written** (requires 30 test cases per story)
  - No unit tests, no integration tests, no component tests, no E2E tests
  - Critical for backup/export functionality

- **All ACs Met**: âŒ Not Met
  - **Completed**: AC 2, 3, 4, 5, 6, 8, 10 (7/15 = 47%)
  - **Partial**: AC 7, 11, 13 (backend only, UI missing)
  - **Incomplete**: AC 1, 9, 12, 14, 15 (38%)
  - **Overall**: ~40% complete

### Requirements Traceability (AC â†’ Implementation)

| AC  | Requirement                     | Status | Notes                                                                                                                           |
| --- | ------------------------------- | ------ | ------------------------------------------------------------------------------------------------------------------------------- |
| 1   | Download button in settings     | âŒ     | No UI implemented                                                                                                               |
| 2   | JSON backup generation          | âœ…     | [backup.service.ts:119-331](apps/web/src/server/services/backup.service.ts#L119-L331)                                           |
| 3   | Well-formatted JSON             | âœ…     | Uses `JSON.stringify(data, null, 2)`                                                                                            |
| 4   | Document metadata with blobURLs | âœ…     | [backup.service.ts:313](apps/web/src/server/services/backup.service.ts#L313)                                                    |
| 5   | Rate limiting enforced          | âœ…     | [rate-limiter.ts](apps/web/src/lib/rate-limiter.ts), [project.ts:421-436](apps/web/src/server/api/routers/project.ts#L421-L436) |
| 6   | Error message for rate limit    | âœ…     | [project.ts:432-435](apps/web/src/server/api/routers/project.ts#L432-L435)                                                      |
| 7   | Backup history displayed        | âš ï¸     | Backend: âœ… [project.ts:517-528](apps/web/src/server/api/routers/project.ts#L517-L528) / UI: âŒ Missing                         |
| 8   | RBAC enforced (owners only)     | âœ…     | [project.ts:418,522,544](apps/web/src/server/api/routers/project.ts#L418)                                                       |
| 9   | Large projects <10s             | âŒ     | Not tested, no timeout protection                                                                                               |
| 10  | Schema version included         | âœ…     | [backup.service.ts:197](apps/web/src/server/services/backup.service.ts#L197)                                                    |
| 11  | JSON/ZIP options                | âš ï¸     | Backend: âœ… [project.ts:406-506](apps/web/src/server/api/routers/project.ts#L406-L506) / UI: âŒ Missing                         |
| 12  | ZIP with JSON + documents       | ðŸ›     | Partial - has bug in blob fetching                                                                                              |
| 13  | ZIP size estimate               | âš ï¸     | Backend: âœ… [project.ts:539-550](apps/web/src/server/api/routers/project.ts#L539-L550) / UI: âŒ Missing                         |
| 14  | Clear UI indication             | âŒ     | No UI implemented                                                                                                               |
| 15  | Progress indicator for ZIP      | âŒ     | No UI implemented                                                                                                               |

### Improvements Checklist

**CRITICAL - Must Fix Before Production:**

- [ ] **Fix blob fetching bug** - Change `backup.service.ts:591` from `getDocumentBlob(doc.id)` to `getDocumentBlob(doc.blobUrl)`
- [ ] **Implement all 30 test cases** - See story Tasks section for complete list
- [ ] **Create ProjectBackupSection component** - With dual download options (JSON/ZIP)
- [ ] **Create BackupHistoryList component** - Display last 10 backups with type badges
- [ ] **Create project settings page** - `/projects/[id]/settings`
- [ ] **Run database migration** - Execute `bunx drizzle-kit generate` and `bunx drizzle-kit push`
- [ ] **Add timeout protection** - Implement 10s timeout for JSON, 60s for ZIP backups
- [ ] **Add ZIP size limit** - Enforce 500MB maximum to prevent memory exhaustion
- [ ] **Update File List** - Populate Dev Agent Record section with modified files

**MEDIUM - Should Address:**

- [ ] Add JSDoc comments for all public methods in BackupService
- [ ] Implement actual blob fetching in ZIP generation (currently placeholder)
- [ ] Add memory threshold protection for large ZIP archives
- [ ] Add performance testing for large projects (1000+ costs)
- [ ] Add concurrent blob fetching with rate limiting (currently sequential)

**LOW - Future Enhancements:**

- [ ] Consider Redis-based distributed rate limiting for multi-server deployments
- [ ] Add password protection for ZIP archives (encryption)
- [ ] Implement backup download cancellation for ZIP generation
- [ ] Add compression level configuration option

### Security Review

**Strengths:**

- âœ… RBAC enforcement correctly implemented using `assertProjectOwner()` helper
- âœ… Rate limiting prevents abuse (5 JSON/hr, 2 ZIP/hr per user)
- âœ… Authorization checks on all backup endpoints
- âœ… No SQL injection vulnerabilities (using Drizzle ORM properly)
- âœ… Input validation with Zod schemas

**Concerns:**

- âš ï¸ **In-memory rate limiter not distributed** - Single server assumption may not hold in production. Could be bypassed with multiple server instances or server restarts.
- âš ï¸ **ZIP archives unencrypted** - Contains sensitive financial data and documents. Users responsible for securing downloaded files. Consider password-protected ZIPs in future.
- â„¹ï¸ **Blob URL exposure** - JSON backups include Netlify Blob URLs. These are time-limited signed URLs, which is acceptable.

**Verdict**: Security implementation is **GOOD** for backend, but needs distributed rate limiting for production scale.

### Performance Considerations

**Issues Found:**

- âŒ **No timeout protection** - Large project backups could hang indefinitely
- âŒ **No ZIP size limit** - Could exhaust server memory with very large projects
- âš ï¸ **Sequential document fetching** - Could be optimized with parallel requests (limit concurrency to 5-10)
- âŒ **AC 9 not validated** - "Large projects (1000+ costs) export <10s" cannot be verified without performance tests

**Positive:**

- âœ… Good use of Drizzle relational queries (single query with joins)
- âœ… JSON.stringify compression ratio estimation (30%) is reasonable
- âœ… Rate limiting prevents resource exhaustion

**Recommendations:**

1. Add `Promise.race()` with timeout for backup generation
2. Implement hard limit check in `estimateZipSize()` - reject if >500MB
3. Add performance tests with mock data (1000+ costs)
4. Consider streaming ZIP generation for archives >100MB

### Files Modified During Review

**No files modified** - Analysis only. Developer must address critical bugs and complete implementation.

**Developer Actions Required:**

1. Fix critical bug in backup.service.ts
2. Implement all UI components
3. Write comprehensive test suite (30 test cases)
4. Run database migration
5. Update File List in story
6. Change story status to "Review" when complete
7. Re-request QA review after all items addressed

### Gate Status

**Gate: FAIL** â†’ [docs/qa/gates/6.2-user-initiated-project-backup-export.yml](docs/qa/gates/6.2-user-initiated-project-backup-export.yml)

**Quality Score: 0/100** (4 high severity issues, 3 medium severity issues)

**Status Reason**: Implementation incomplete with critical bugs. Work appears to be in progress - backend ~40% complete, no UI, no tests, database migration not run. Critical bug in ZIP blob fetching logic.

**Top Issues Summary:**

1. ðŸ”´ **BUG-001** (HIGH): Critical blob fetching bug will break ZIP downloads
2. ðŸ”´ **TEST-001** (HIGH): Zero test coverage (0/30 test cases)
3. ðŸ”´ **IMPL-001** (HIGH): UI components not implemented
4. ðŸ”´ **IMPL-002** (HIGH): Database migration not executed
5. ðŸŸ¡ **PERF-001** (MEDIUM): No timeout protection
6. ðŸŸ¡ **PERF-002** (MEDIUM): No ZIP size limit
7. ðŸŸ¡ **ARCH-001** (MEDIUM): In-memory rate limiter not distributed

### Recommended Status

**âŒ NOT Ready for Done** - Story incomplete and has critical bugs

**Next Steps:**

1. Developer must complete all implementation tasks
2. Fix critical bug in blob fetching
3. Implement all 30 test cases
4. Create all UI components
5. Run database migration
6. Verify all 15 acceptance criteria
7. Update File List
8. Change status to "Review"
9. Request new QA review

**Estimated Additional Work**: 8-12 hours

- Bug fix: 15 minutes
- UI components: 3-4 hours
- Test implementation: 4-6 hours
- Migration + verification: 30 minutes
- Documentation: 30 minutes

---

**Note to Developer**: Excellent architectural work on the backend! The design is solid and the code quality is high. However, this story is only ~40% complete. Please finish the implementation, write comprehensive tests, and fix the critical blob fetching bug before requesting another QA review. Happy to review again once complete!
